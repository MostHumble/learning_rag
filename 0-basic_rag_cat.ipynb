{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ngxson/demo_simple_rag_py/resolve/main/cat-facts.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGdTsyhtTbVF",
        "outputId": "04609f01-7198-4be2-eee3-95c9883fdd95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 19:43:22--  https://huggingface.co/ngxson/demo_simple_rag_py/resolve/main/cat-facts.txt\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.97, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22657 (22K) [text/plain]\n",
            "Saving to: ‘cat-facts.txt.1’\n",
            "\n",
            "cat-facts.txt.1     100%[===================>]  22.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-18 19:43:22 (148 MB/s) - ‘cat-facts.txt.1’ saved [22657/22657]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Model names\n",
        "EMBEDDING_MODEL_NAME = 'BAAI/bge-large-zh-v1.5'\n",
        "LANGUAGE_MODEL_NAME = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(LANGUAGE_MODEL_NAME)\n",
        "language_model = AutoModelForCausalLM.from_pretrained(LANGUAGE_MODEL_NAME).to(device)"
      ],
      "metadata": {
        "id": "sg5kkWtobHHP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval\n",
        "def retrieve(query, top_n=3):\n",
        "    query_embedding = torch.tensor(\n",
        "        embedding_model.encode(query, normalize_embeddings=True), dtype=torch.float32\n",
        "    )\n",
        "    similarities = F.cosine_similarity(VECTOR_DB, query_embedding.unsqueeze(0))\n",
        "    top_indices = similarities.topk(top_n).indices.tolist()\n",
        "    return [(CHUNKS[i], similarities[i].item()) for i in top_indices]"
      ],
      "metadata": {
        "id": "eabI23XabMJb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "with open('cat-facts.txt', 'r') as file:\n",
        "    dataset = [line.strip() for line in file if line.strip()]\n",
        "    print(f'Loaded {len(dataset)} entries')\n",
        "\n",
        "# Build vector DB\n",
        "CHUNKS = []\n",
        "embeddings = []\n",
        "\n",
        "print(\"Encoding chunks:\")\n",
        "for chunk in tqdm(dataset):\n",
        "    embedding = embedding_model.encode(chunk, normalize_embeddings=True)\n",
        "    embeddings.append(embedding)\n",
        "    CHUNKS.append(chunk)\n",
        "\n",
        "VECTOR_DB = torch.tensor(np.array(embeddings), dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djDPamGjbJOV",
        "outputId": "31d4d11d-3152-4c93-e2b5-ab935a906c9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 150 entries\n",
            "Encoding chunks:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:04<00:00, 34.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2X-PFzETSD8",
        "outputId": "5f099f72-7dd9-4889-edb5-ec19c3cd181d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask me a question: How many years do cats live?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved knowledge:\n",
            " - (similarity: 0.72) When well treated, a cat can live twenty or more years but the average life span of a domestic cat is 14 years.\n",
            " - (similarity: 0.68) On average, cats spend 2/3 of every day sleeping. That means a nine-year-old cat has been awake for only three years of its life.\n",
            " - (similarity: 0.68) Neutering a cat extends its life span by two or three years.\n",
            "\n",
            "Chatbot response:\n",
            "\n",
            "assistant\n",
            "The average life span of a domestic cat is 14 years. However, the life span of a cat can vary depending on factors such as the cat's breed, lifestyle, and overall health.\n"
          ]
        }
      ],
      "source": [
        "# Chat\n",
        "input_query = input('Ask me a question: ')\n",
        "retrieved_knowledge = retrieve(input_query)\n",
        "\n",
        "print('Retrieved knowledge:')\n",
        "for chunk, similarity in retrieved_knowledge:\n",
        "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
        "\n",
        "context = '\\n'.join([f'- {chunk}' for chunk, _ in retrieved_knowledge])\n",
        "system_prompt = \"You are a helpful chatbot.\\nUse only the following context to answer the user's question:\\n\" + context\n",
        "\n",
        "# Chat-style formatting\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": input_query},\n",
        "]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate\n",
        "outputs = language_model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print('\\nChatbot response:\\n')\n",
        "print(response.split(input_query)[-1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful chatbot.\\nUse only the following context to answer the user's question, do not add any additional information, only constrain yourself to those provided:\\n\" + context\n",
        "\n",
        "# Chat-style formatting\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": input_query},\n",
        "]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate\n",
        "outputs = language_model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print('\\nChatbot response:\\n')\n",
        "print(response.split(input_query)[-1].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T30sqIOUZkz4",
        "outputId": "14feb6e4-b295-485e-d664-30955057e099"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chatbot response:\n",
            "\n",
            "assistant\n",
            "On average, a cat can live between 14 to 20 years, but the average life span of a domestic cat is 14 years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful chatbot.\\nUse only the following context to answer the user's question, do not add any additional information, only constrain yourself to those provided, but be reasonable:\\n\" + context\n",
        "\n",
        "# Chat-style formatting\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": input_query},\n",
        "]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate\n",
        "outputs = language_model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=300,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print('\\nChatbot response:\\n')\n",
        "print(response.split(input_query)[-1].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG6PI2GgaU-m",
        "outputId": "b833567d-ffc1-4274-97c1-4bc6178cbbd1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chatbot response:\n",
            "\n",
            "assistant\n",
            "On average, cats live 14 years.\n"
          ]
        }
      ]
    }
  ]
}